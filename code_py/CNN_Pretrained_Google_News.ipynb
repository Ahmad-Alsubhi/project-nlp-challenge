{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hP0qCN9HcMcG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, GlobalAveragePooling1D\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "import nltk\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe-ey1zldhhV",
        "outputId": "b7d09fe2-57d2-448f-b012-8d03e334f30a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H-8yHHoJcMcJ"
      },
      "outputs": [],
      "source": [
        "full_df = pd.read_csv('/content/data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8A_hL2aJcMcP"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1sC_ALJicMcP"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenization\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]  # Lemmatization\n",
        "    return ' '.join(lemmatized_tokens) # Return tokenized list\n",
        "\n",
        "# Apply preprocessing on title and text\n",
        "full_df['clean_text'] = full_df['title'] + \" \" + full_df['text']\n",
        "full_df['clean_text'] = full_df['clean_text'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P2mvqsJQ4Pu",
        "outputId": "c3fd87f3-4b79-45ea-dcee-eb5059999d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique words: 205333\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "all_words = list(itertools.chain(*full_df['clean_text'].apply(lambda x: x.split())))\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "print(\"Total unique words:\", len(word_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdW7joSbRUnb",
        "outputId": "09f17e40-80ab-4989-850d-8fd1fcb0c87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rare words (appear <5 times): 161602\n"
          ]
        }
      ],
      "source": [
        "rare_words = [word for word, count in word_counts.items() if count < 5]\n",
        "print(f\"Number of rare words (appear <5 times): {len(rare_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmBkTUuTRhkP",
        "outputId": "13a81e8b-1ae8-45d9-ba7e-1150cd4c02d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suggested max_features to cover 95%: 19817\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "word_freq_values = np.array(sorted(word_counts.values(), reverse=True))\n",
        "cumulative_freq = np.cumsum(word_freq_values) / np.sum(word_freq_values)\n",
        "\n",
        "max_features = np.argmax(cumulative_freq >= 0.95) + 1\n",
        "print(f\"Suggested max_features to cover 95%: {max_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWv0QgXehssb"
      },
      "source": [
        "# Pretrained Word2Vec (Google News)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QybhMjvbhNOW",
        "outputId": "312c2a65-9a10-4675-f9b6-407e290b1282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Google News Word2Vec model...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['word2vec_google_news.pkl']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "# Load Pretrained Word2Vec\n",
        "print(\"Loading Google News Word2Vec model...\")\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "joblib.dump(word2vec_model, 'word2vec_google_news.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Sxzk8s7hhr42"
      },
      "outputs": [],
      "source": [
        "# Split data before applying embeddings\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    full_df['clean_text'], full_df['label'], test_size=0.2, random_state=42, stratify=full_df['label']\n",
        ")\n",
        "\n",
        "# Convert words to embeddings & pad sequences\n",
        "max_len = 700  # Maximum sequence length\n",
        "\n",
        "def get_sentence_embedding(tokens, model, vector_size=300):\n",
        "    word_vectors = [model[word] for word in tokens if word in model]\n",
        "\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros((max_len, vector_size))  # Return zero-matrix if no valid words\n",
        "\n",
        "    word_vectors = np.array(word_vectors)\n",
        "\n",
        "    # Truncate if longer than max_len, pad if shorter\n",
        "    if len(word_vectors) > max_len:\n",
        "        word_vectors = word_vectors[:max_len]  # Truncate\n",
        "    else:\n",
        "        pad_size = max_len - len(word_vectors)\n",
        "        word_vectors = np.vstack([word_vectors, np.zeros((pad_size, vector_size))])  # Pad with zeros\n",
        "\n",
        "    return word_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_hEUQjgie8D"
      },
      "outputs": [],
      "source": [
        "# Convert text to embeddings\n",
        "X_train = np.array([get_sentence_embedding(tokens, word2vec_model, 300) for tokens in X_train_text])\n",
        "X_test = np.array([get_sentence_embedding(tokens, word2vec_model, 300) for tokens in X_test_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZQ5kU6ciz7_"
      },
      "outputs": [],
      "source": [
        "# Build Conv1D Model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(max_len, 300)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(len(set(y_train)), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8e3Nqmki4Ff"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hestGZkqi89V"
      },
      "outputs": [],
      "source": [
        "test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Conv1D Model Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9f5tjwlHcmZ"
      },
      "outputs": [],
      "source": [
        "prdictions = model.predict(X_test)\n",
        "print(classification_report(y_test, prdictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkt6KcpwwRmH"
      },
      "outputs": [],
      "source": [
        "model.save(\"conv1d_google_embeddings_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
